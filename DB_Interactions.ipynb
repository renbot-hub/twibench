{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf1eba4",
   "metadata": {},
   "source": [
    "This notebook is used to convert all the data from the multiple datasources (csv / json and other tabulars) to relational model.\\\n",
    "The RDBMS used is PostgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59958a56",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e025b",
   "metadata": {},
   "source": [
    "Imports.\\\n",
    "PySpark is used to manipulate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf0b8ff-8d8e-44b4-97cb-b99faf13c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, unix_timestamp , to_timestamp, regexp_replace\n",
    "import os\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a577d69",
   "metadata": {},
   "source": [
    "PySpark session and link to access it.\\\n",
    "Do NOT forget to forward **port 4040** if using SSH.\\\n",
    "http://localhost:4040\n",
    "\n",
    "Replace DATA_PATH with the path towards Twitter data as downloaded from Botometer\\\n",
    "Replace JDBC_DRIVER_PATH with the path towards postgresql drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/12 15:21:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI available at http://localhost:4040\n",
      "/data2/petit/data/twitter/MGTAB\n",
      "/data2/petit/data/twitter/cresci-2015\n",
      "/data2/petit/data/twitter/cresci-rtbust\n",
      "/data2/petit/data/twitter/gilani-2017\n",
      "/data2/petit/data/twitter/midterm-2018\n",
      "/data2/petit/data/twitter/Twibot-20\n",
      "/data2/petit/data/twitter/other\n",
      "/data2/petit/data/twitter/Twibot-22\n",
      "/data2/petit/data/twitter/cresci-2017\n",
      "/data2/petit/data/twitter/botometer-feedback-2019\n"
     ]
    }
   ],
   "source": [
    "jdbc_driver_path = \"/data2/petit/code/drivers/postgresql-42.7.3.jar\"\n",
    "data_path = \"/data2/petit/data/twitter\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL basic example\") \\\n",
    "    .config(\"spark.driver.memory\", \"30g\") \\\n",
    "    .config(\"spark.executor.memory\", \"190g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.jars\", jdbc_driver_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark UI available at http://localhost:4040\")\n",
    "\n",
    "# set timeparser to legacy to avoid errors when converting timestamps\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    for name in dirs:\n",
    "        print(os.path.join(root, name))\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcaa4f5",
   "metadata": {},
   "source": [
    "# Gilani-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "920942cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gilani_path = os.path.join(data_path, \"gilani-2017\")\n",
    "\n",
    "gilani_users = spark.read.json(os.path.join(gilani_path, \"gilani-2017_tweets.json\"))\\\n",
    "    .select(\"user.*\")\\\n",
    "    .drop(\"entities\") \\\n",
    "    .withColumn(\"created_at\", to_timestamp(col(\"created_at\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\\\n",
    "    .withColumnRenamed(\"id\", \"user_id\")\\\n",
    "\n",
    "gilani_label = spark.read.csv(os.path.join(gilani_path, \"gilani-2017.tsv\"), sep=\"\\t\", header=False)\\\n",
    "    .withColumnRenamed(\"_c0\", \"user_id\")\\\n",
    "    .withColumnRenamed(\"_c1\", \"label\")\n",
    "\n",
    "gilani_users.join(gilani_label, on=\"user_id\", how=\"inner\")\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"gilani_2017\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7106d327",
   "metadata": {},
   "source": [
    "# Midterm-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514ce17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/petit/data/twitter/midterm-2018/midterm-2018_processed_user_objects.json\n",
      "/data2/petit/data/twitter/midterm-2018/midterm-2018.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "midterm_path = os.path.join(data_path, \"midterm-2018\")\n",
    "\n",
    "midterm_users = spark.read.json(os.path.join(midterm_path, \"midterm-2018_processed_user_objects.json\"))\\\n",
    "    .withColumn(\"user_created_at\", to_timestamp(col(\"user_created_at\"), \"EEE MMM dd HH:mm:ss yyyy\"))\\\n",
    "\n",
    "midterm_label = spark.read.csv(os.path.join(midterm_path, \"midterm-2018.tsv\"), sep=\"\\t\", header=False)\\\n",
    "    .withColumnRenamed(\"_c0\", \"user_id\")\\\n",
    "    .withColumnRenamed(\"_c1\", \"label\")\\\n",
    "\n",
    "midterm_users.join(midterm_label, on=\"user_id\", how=\"inner\")\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"midterm_2018\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c87f79",
   "metadata": {},
   "source": [
    "# Cresci-rtbust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d6cf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crescirtb_path = os.path.join(data_path, \"cresci-rtbust\")\n",
    "\n",
    "crescirtb_users = spark.read.json(os.path.join(crescirtb_path, \"cresci-rtbust-2019_tweets.json\"))\\\n",
    "    .select(\"user.*\")\\\n",
    "    .drop(\"entities\")\\\n",
    "    .withColumn(\"created_at\", to_timestamp(col(\"created_at\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\\\n",
    "    .withColumnRenamed(\"id\", \"user_id\")\n",
    "\n",
    "crescirtb_label = spark.read.csv(os.path.join(crescirtb_path, \"cresci-rtbust-2019.tsv\"), sep=\"\\t\", header=False)\\\n",
    "    .withColumnRenamed(\"_c0\", \"user_id\")\\\n",
    "    .withColumnRenamed(\"_c1\", \"label\")\n",
    "\n",
    "crescirtb_users.join(crescirtb_label, on=\"user_id\", how=\"inner\")\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"cresci_rtbust\", mode=\"overwrite\", properties=connection_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5f457",
   "metadata": {},
   "source": [
    "# Botometer-Feedback-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39970d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "botometerfb_path = os.path.join(data_path, \"botometer-feedback-2019\")\n",
    "\n",
    "botometerfb_users = spark.read.json(os.path.join(botometerfb_path, \"botometer-feedback-2019_tweets.json\"))\\\n",
    "    .select(\"user.*\")\\\n",
    "    .drop(\"entities\")\\\n",
    "    .withColumn(\"created_at\", to_timestamp(col(\"created_at\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\\\n",
    "    .withColumnRenamed(\"id\", \"user_id\")\n",
    "\n",
    "botometerfb_label = spark.read.csv(os.path.join(botometerfb_path, \"botometer-feedback-2019.tsv\"), sep=\"\\t\", header=False)\\\n",
    "    .withColumnRenamed(\"_c0\", \"user_id\")\\\n",
    "    .withColumnRenamed(\"_c1\", \"label\")\n",
    "\n",
    "botometerfb_users.join(botometerfb_label, on=\"user_id\", how=\"inner\")\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"botometer_feedback\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec6e9d",
   "metadata": {},
   "source": [
    "# Cresci-2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ad611",
   "metadata": {},
   "source": [
    "For this one, the bots and humans are spread between multiple directories.\n",
    "\n",
    "We need to read from each separate file (using *) before merging them.\n",
    "\n",
    "Encoding is weird, the regex fixes those issues. (both cresci-2015 and cresci-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa770d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/22 09:56:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cresci2015_path = os.path.join(data_path, \"cresci-2015\")\n",
    "\n",
    "bot_dirs = [\"FSF\", \"INT\", \"TWT\"]\n",
    "human_dirs = [\"E13\", \"TFP\"]\n",
    "\n",
    "spark.read.csv(os.path.join(cresci2015_path, \"*/users.csv\"), header=True)\\\n",
    "        .withColumn(\"created_at\", to_timestamp(col(\"created_at\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\\\n",
    "        .withColumn(\"label\", when(col(\"dataset\").isin(bot_dirs), \"bot\").otherwise(\"human\"))\\\n",
    "        .withColumn(\"name\", regexp_replace(col(\"name\"), \"[^ -~]\", \"\"))\\\n",
    "        .withColumn(\"screen_name\", regexp_replace(col(\"screen_name\"), \"[^ -~]\", \"\"))\\\n",
    "        .withColumn(\"description\", regexp_replace(col(\"description\"), \"[^ -~]\", \"\"))\\\n",
    "        .withColumn(\"location\", regexp_replace(col(\"location\"), \"[^ -~]\", \"\"))\\\n",
    "        .withColumn(\"url\", regexp_replace(col(\"url\"), \"[^ -~]\", \"\"))\\\n",
    "        .withColumnRenamed(\"id\", \"user_id\")\\\n",
    "        .write.jdbc(url=jdbc_url, table=\"cresci_2015_users\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a02a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.csv(os.path.join(cresci2015_path, \"*/tweets.csv\"), header=True)\\\n",
    "        .withColumn(\"created_at\", to_timestamp(col(\"created_at\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\\\n",
    "        .select([regexp_replace(col(c), '\\x00', '').alias(c) for c in spark.read.csv(os.path.join(cresci2015_path, \"*/tweets.csv\"), header=True).columns])\\\n",
    "        .write.jdbc(url=jdbc_url, table=\"cresci_2015_tweets\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5c4fa4",
   "metadata": {},
   "source": [
    "# Cresci-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb84a83",
   "metadata": {},
   "source": [
    "This one is a bit weird. Tweets and Users CSV files in the various directories don't share all the exact same columns (mostly \"updated\" and \"crawled_at\")\n",
    "\n",
    "So first we need to clean them by keeping only the columns in common before merging them all and pushing them in the DB.\n",
    "\n",
    "First cell is users, second cell is tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a82a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cresci2017_path = os.path.join(data_path, \"cresci-2017\")\n",
    "\n",
    "bot_dirs = [\"fake_followers\", \"social_spambots_1\", \"social_spambots_2\", \"social_spambots_3\"]\n",
    "human_dirs = [\"genuine_accounts\"]\n",
    "\n",
    "# Load all the users DataFrames\n",
    "genuine_accounts_users = spark.read.csv(os.path.join(cresci2017_path, \"genuine_accounts/users.csv\"), header=True)\n",
    "fake_followers_users = spark.read.csv(os.path.join(cresci2017_path, \"fake_followers/users.csv\"), header=True)\n",
    "social_spambots_1_users = spark.read.csv(os.path.join(cresci2017_path, \"social_spambots_1/users.csv\"), header=True)\n",
    "social_spambots_2_users = spark.read.csv(os.path.join(cresci2017_path, \"social_spambots_2/users.csv\"), header=True)\n",
    "social_spambots_3_users = spark.read.csv(os.path.join(cresci2017_path, \"social_spambots_3/users.csv\"), header=True)\n",
    "\n",
    "# Find the common columns in all the DataFrames (using set intersection operation)\n",
    "common_columns = set(genuine_accounts_users.columns)\n",
    "common_columns &= set(fake_followers_users.columns)\n",
    "common_columns &= set(social_spambots_1_users.columns)\n",
    "common_columns &= set(social_spambots_2_users.columns)\n",
    "common_columns &= set(social_spambots_3_users.columns)\n",
    "\n",
    "# Select only the common columns \n",
    "genuine_accounts_users = genuine_accounts_users.select(*common_columns)\n",
    "fake_followers_users = fake_followers_users.select(*common_columns)\n",
    "social_spambots_1_users = social_spambots_1_users.select(*common_columns)\n",
    "social_spambots_2_users = social_spambots_2_users.select(*common_columns)\n",
    "social_spambots_3_users = social_spambots_3_users.select(*common_columns)\n",
    "\n",
    "# Add the label column \n",
    "genuine_accounts_users = genuine_accounts_users.withColumn(\"label\", when(col(\"id\").isNotNull(), \"human\"))\n",
    "fake_followers_users = fake_followers_users.withColumn(\"label\", when(col(\"id\").isNotNull(), \"bot\"))\n",
    "social_spambots_1_users = social_spambots_1_users.withColumn(\"label\", when(col(\"id\").isNotNull(), \"bot\"))\n",
    "social_spambots_2_users = social_spambots_2_users.withColumn(\"label\", when(col(\"id\").isNotNull(), \"bot\"))\n",
    "social_spambots_3_users = social_spambots_3_users.withColumn(\"label\", when(col(\"id\").isNotNull(), \"bot\"))\n",
    "\n",
    "# Merge all the DataFrames into a single DataFrame\n",
    "final_df = genuine_accounts_users.union(fake_followers_users) \\\n",
    "                                .union(social_spambots_1_users) \\\n",
    "                                .union(social_spambots_2_users) \\\n",
    "                                .union(social_spambots_3_users)\n",
    "\n",
    "final_df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\\\n",
    "        .withColumnRenamed(\"id\", \"user_id\")\\\n",
    "        .write.jdbc(url=jdbc_url, table=\"cresci_2017_users\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24df6b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load all the tweets DataFrames\n",
    "genuine_accounts_tweets = spark.read.csv(os.path.join(cresci2017_path, \"genuine_accounts/tweets.csv\"), header=True)\n",
    "fake_followers_tweets = spark.read.csv(os.path.join(cresci2017_path, \"fake_followers/tweets.csv\"), header=True)\n",
    "social_spambots_1_tweets = spark.read.csv(os.path.join(cresci2017_path, \"social_spambots_1/tweets.csv\"), header=True)\n",
    "social_spambots_2_tweets = spark.read.csv(os.path.join(cresci2017_path, \"social_spambots_2/tweets.csv\"), header=True)\n",
    "social_spambots_3_tweets = spark.read.csv(os.path.join(cresci2017_path, \"social_spambots_3/tweets.csv\"), header=True)\n",
    "\n",
    "# Find the common columns in all the DataFrames (using set intersection operation)\n",
    "common_columns = set(genuine_accounts_tweets.columns)\n",
    "common_columns &= set(fake_followers_tweets.columns)\n",
    "common_columns &= set(social_spambots_1_tweets.columns)\n",
    "common_columns &= set(social_spambots_2_tweets.columns)\n",
    "common_columns &= set(social_spambots_3_tweets.columns)\n",
    "\n",
    "# Select only the common columns\n",
    "genuine_accounts_tweets = genuine_accounts_tweets.select(*common_columns)\n",
    "fake_followers_tweets = fake_followers_tweets.select(*common_columns)\n",
    "social_spambots_1_tweets = social_spambots_1_tweets.select(*common_columns)\n",
    "social_spambots_2_tweets = social_spambots_2_tweets.select(*common_columns)\n",
    "social_spambots_3_tweets = social_spambots_3_tweets.select(*common_columns)\n",
    "\n",
    "# Merge all the DataFrames into a single DataFrame\n",
    "final_df = genuine_accounts_tweets.union(fake_followers_tweets) \\\n",
    "                                .union(social_spambots_1_tweets) \\\n",
    "                                .union(social_spambots_2_tweets) \\\n",
    "                                .union(social_spambots_3_tweets)\n",
    "\n",
    "final_df.withColumn(\"created_at\", to_timestamp(col(\"created_at\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\\\n",
    "        .select([regexp_replace(col(c), '\\x00', '').alias(c) for c in final_df.columns])\\\n",
    "        .write.jdbc(url=jdbc_url, table=\"cresci_2017_tweets\", mode=\"overwrite\", properties=connection_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0432e",
   "metadata": {},
   "source": [
    "# TwiBot20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de30b23",
   "metadata": {},
   "source": [
    "### Write in DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea976aa",
   "metadata": {},
   "source": [
    "Twibot20 dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af042e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb20_path = \"/data2/petit/data/twitter/Twibot-20/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5fc47",
   "metadata": {},
   "source": [
    "Reading users and tweets from JSON in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96b4d44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_nodes = spark.read \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .json(tb20_path + \"node.json\") \\\n",
    "    .persist()\n",
    "\n",
    "#df_nodes.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fded0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in df_users ids that start with the letter \"t\"\n",
    "df_users = df_nodes.filter(df_nodes.id.startswith(\"u\")).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a179744a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_users.select('*',\"public_metrics.*\") \\\n",
    "    .drop(\"public_metrics\",\"entities\",\"pinned_tweet_id\",\"profile_image_url\",\"url\",\"withheld\",\"text\") \\\n",
    "    .withColumn(\"created_at\", to_timestamp(col(\"created_at\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"petit.tb20_users\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4493c8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_tweets = df_nodes.filter(df_nodes.id.startswith(\"t\")) \\\n",
    "    .select(\"id\",\"text\") \\\n",
    "    .withColumn(\"is_retweet\", when(col(\"text\").startswith(\"RT\"), \"true\").otherwise(\"false\"))\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"petit.tb20_tweets\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ccdb7",
   "metadata": {},
   "source": [
    "Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2264020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(tb20_path + \"edge.csv\") \\\n",
    "    .write.jdbc(url=jdbc_url, table=\"petit.tb20_edges\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201a312",
   "metadata": {},
   "source": [
    "Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d0c1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edge = spark.read.jdbc(url=jdbc_url, table=\"petit.tb20_edges\", properties=connection_properties)\n",
    "df_edge.createOrReplaceTempView('edge')\n",
    "\n",
    "df_tweet = spark.read.jdbc(url=jdbc_url, table=\"petit.tb20_tweets\", properties=connection_properties)\n",
    "df_tweet.createOrReplaceTempView('tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79fe7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = spark.sql(\"SELECT source_id, target_id FROM edge WHERE relation = 'post'\")\n",
    "df_retweet = spark.sql(\"SELECT * FROM tweet WHERE is_retweet = 'true'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c3aa322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# count each user retweets\n",
    "df_post.join(df_retweet, df_post.target_id == df_retweet.id)\\\n",
    "    .groupBy(\"source_id\")\\\n",
    "    .count()\\\n",
    "    .withColumnRenamed(\"source_id\", \"user_id\")\\\n",
    "    .withColumnRenamed(\"count\", \"retweet_count\")\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"petit.tb20_user_retweet\", mode=\"overwrite\", properties=connection_properties)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776e43c",
   "metadata": {},
   "source": [
    "# TwiBot22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412b393",
   "metadata": {},
   "source": [
    "## Original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa989749",
   "metadata": {},
   "source": [
    "The original dataset is split between multiple files in different formats : json and csv.\\\n",
    "This part aims at writing them in a relational database.\n",
    "\n",
    "The **database name** is *tep2022*, the **schema** is *petit*, and those **twibot22 tables** start with *tb22_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4d568bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb22_path = \"/data2/petit/data/twibot22/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54163e17",
   "metadata": {},
   "source": [
    "Load the 9 tweet jsons in the database.\\\n",
    "Careful, very time consuming. Count ~8-9 minutes per file, roughly 75 minutes in total.\\\n",
    "Columns are listed below.\\\n",
    "*id, author_id, created_ad, text, lang, in_reply_to_user_id, like_count, quote_count, reply_count, retweet_count,*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db0a8ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for i in range(0,9):\n",
    "    spark.read.option(\"multiLine\", True).json(tb22_path + \"/tweet_\" + str(i) + \".json\") \\\n",
    "        .select(\"id\",\"author_id\",\"created_at\",\"text\",\"lang\",\"in_reply_to_user_id\",\"public_metrics.*\") \\\n",
    "        .withColumn(\"id\", col(\"id\").cast(\"string\")) \\\n",
    "        .withColumn(\"author_id\", col(\"author_id\").cast(\"string\")) \\\n",
    "        .withColumn(\"created_at\", col(\"created_at\").cast(\"timestamp\")) \\\n",
    "        .withColumn(\"text\", col(\"text\").cast(\"string\")) \\\n",
    "        .withColumn(\"lang\", col(\"lang\").cast(\"string\")) \\\n",
    "        .withColumn(\"in_reply_to_user_id\", col(\"in_reply_to_user_id\").cast(\"string\")) \\\n",
    "        .withColumn(\"quote_count\", col(\"quote_count\").cast(\"integer\")) \\\n",
    "        .withColumn(\"reply_count\", col(\"reply_count\").cast(\"integer\")) \\\n",
    "        .withColumn(\"retweet_count\", col(\"retweet_count\").cast(\"integer\")) \\\n",
    "        .withColumn(\"like_count\", col(\"like_count\").cast(\"integer\")) \\\n",
    "        .write.jdbc(url=jdbc_url, table=\"petit.tb22_tweet\", mode=\"append\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3f5b9",
   "metadata": {},
   "source": [
    "Loads users in the database. Columns are listed below.\\\n",
    "*created_at, description, id, location, name, protected, username, verified, followers_count, following_count, listed_count, tweet_count*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d30caff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"multiLine\", True).json(tb22_path + \"/user.json\")\\\n",
    "    .select('*',\"public_metrics.*\") \\\n",
    "    .drop(\"public_metrics\",\"entities\",\"pinned_tweet_id\",\"profile_image_url\",\"url\",\"withheld\") \\\n",
    "    .withColumn(\"created_at\", col(\"created_at\").cast(\"timestamp\")) \\\n",
    "    .withColumn(\"description\", when(col(\"description\").contains(\"\\u0000\"), \"\").otherwise(col(\"description\")))\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"petit.tb22_user\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d54cf",
   "metadata": {},
   "source": [
    "Loads edges of the graph in the database. Columns are listed below.\\\n",
    "*source_id, relation, target_id*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e4304c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(tb22_path + \"/edge.csv\", header=True)\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"petit.tb22_edge\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec1ccc",
   "metadata": {},
   "source": [
    "Loads the labels and train/test/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8c3ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(tb22_path + \"/label.csv\", header=True)\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"petit.tb22_label\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5a3d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(tb22_path + \"/split.csv\", header=True)\\\n",
    "    .write.jdbc(url=jdbc_url, table=\"petit.tb22_split\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e8fc1",
   "metadata": {},
   "source": [
    "## Interaction graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc323d40",
   "metadata": {},
   "source": [
    "Get edge table with all relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac39d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edge = spark.read.jdbc(url=jdbc_url, table=\"petit.tb22_edge\", properties=connection_properties)\n",
    "df_edge.createOrReplaceTempView('edge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2997292",
   "metadata": {},
   "source": [
    "Split all the different edge categories that will be used into different dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bdfc8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt = spark.sql(\"SELECT source_id, target_id FROM edge WHERE relation = 'retweeted'\")\n",
    "df_post = spark.sql(\"SELECT source_id, target_id FROM edge WHERE relation = 'post'\")\n",
    "df_reply = spark.sql(\"SELECT source_id, target_id FROM edge WHERE relation = 'replied_to'\")\n",
    "df_mention = spark.sql(\"SELECT source_id, target_id FROM edge WHERE relation = 'mentioned'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357427c7",
   "metadata": {},
   "source": [
    "### Retweet graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd9953",
   "metadata": {},
   "source": [
    "Obtain user retweets\\\n",
    "Source user retweeted from Target user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "246565f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_a = df_post.alias(\"df_post_a\") \n",
    "df_post_b = df_post.alias(\"df_post_b\")\n",
    "\n",
    "df_rt_a = df_rt.alias(\"df_rt_a\")\n",
    "\n",
    "df_user_retweet = df_post_a.join(df_rt_a, col('df_post_a.target_id') == col('df_rt_a.source_id'))\\\n",
    "    .join(df_post_b, col('df_rt_a.target_id') == col('df_post_b.target_id'))\\\n",
    "    .select(col('df_post_a.source_id').alias('source_id'), col('df_post_b.source_id').alias('target_id'))\\\n",
    "    .persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c8606",
   "metadata": {},
   "source": [
    "Count duplicate rows (users that retweeted from the same user multiple times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "828ede97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_retweet = df_user_retweet\\\n",
    "    .groupBy('source_id','target_id')\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cd988c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_retweet.write\\\n",
    "    .jdbc(url=jdbc_url, table=\"petit.tb22_retweet\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5b5a3",
   "metadata": {},
   "source": [
    "### Reply graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf81293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_a = df_post.alias(\"df_post_a\")\n",
    "df_post_b = df_post.alias(\"df_post_b\")\n",
    "\n",
    "df_reply_a = df_reply.alias(\"df_reply_a\")\n",
    "\n",
    "df_user_reply = df_post_a.join(df_reply_a, col('df_post_a.target_id') == col('df_reply_a.source_id'))\\\n",
    "    .join(df_post_b, col('df_reply_a.target_id') == col('df_post_b.target_id'))\\\n",
    "    .select(col('df_post_a.source_id').alias('source_id'), col('df_post_b.source_id').alias('target_id'))\\\n",
    "    .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94da1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_reply = df_user_reply\\\n",
    "    .groupBy('source_id','target_id')\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ad3cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_reply.write\\\n",
    "    .jdbc(url=jdbc_url, table=\"petit.tb22_reply\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb42f7",
   "metadata": {},
   "source": [
    "### Mention graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aed64d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_a = df_post.alias(\"df_post_a\")\n",
    "\n",
    "df_mention_a = df_mention.alias(\"df_mention_a\")\n",
    "\n",
    "df_user_mention = df_post_a.join(df_mention_a, col('df_post_a.target_id') == col('df_mention_a.source_id'))\\\n",
    "    .select(col('df_post_a.source_id').alias('source_id'), col('df_mention_a.target_id').alias('target_id'))\\\n",
    "    .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57c73f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_mention = df_user_mention\\\n",
    "    .groupBy('source_id','target_id')\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6eabeab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_mention.write\\\n",
    "    .jdbc(url=jdbc_url, table=\"petit.tb22_mention\", mode=\"overwrite\", properties=connection_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479fcfd5",
   "metadata": {},
   "source": [
    "# Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3368e47",
   "metadata": {},
   "source": [
    "Run the **stop** function if needed to force shutdown the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b92e67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
